---
title: "Life History Strategies in Lizards: Insights into
the Slow-Fast Continuum"
Author: Eamon O Cathain and Richard Slevin
output: html_notebook
---

# Set Up
## Initilisation
### Load Libraries
```{r}
library(tidyverse)
library(corrplot) # For correlation plots
library(FactoMineR) # For PCA
library(factoextra) # For PCA plots
library(vegan) # For CCA
library(ggplot2)
library(rsample) # For sampling of data (split, analysis,..)
library(rpart)
library(rpart.plot)
library(randomForest)
library(tibble)
library(tidyr)
library(gridExtra)
library(factoextra)
library(caret)
```

### Knitting Configuration
```{r}
# Set to render chunks during the knitting process
knitr::opts_chunk$set(echo = TRUE)
```


### Set Up the Environment
```{r}
# Clear Environment
rm(list = ls())

# Set Working Directory
setwd("/Users/eamon/Desktop/MathToolsProject/R_Project/MathematicalTools/")

# Load Data
data <- read.csv("lizard.csv")
```

### View the Data
```{r}
str(data)
```

## Cleaning
### Change Variable Types
```{r}
# This variables was input as a character, but it should be numeric
data$SD.Female.adult.weight..g. <- as.numeric(data$SD.Female.adult.weight..g.)
```

### Set Blanks in Categorical Variables as NA
```{r}
# Convert the blank values of categorical variables to NA
data<-data %>%
  mutate(across(where(is.character), ~ na_if(.x, ""))) %>%
  mutate(across(where(is.character), as.factor))
```

### Check for and Remove Duplicates
```{r}
# Save duplicates as an object
duplicates <- data[duplicated(data), ]

# Remove duplicates
data <- data[!duplicated(data), ]
```

### Visualise the Proportion of Each Variable Which is NA
By doing this we found that most of the numeric variables had a high proportion of NA's, with some such as RCM having over 50%. Therefore exclusion of all rows containing NA's was likely to drastically reduce the size dataset and cause data loss. The number of rows in the original dataset was 734 and the number after exclusion of all NAs was 146. Therefore the NA's were removed separately for each analysis using only the variables required for that analysis, which helped to reduce data loss.
```{r}
# Calculate percentage of NA values for each column
na_percentage <- colSums(is.na(data)) / nrow(data) * 100

# Convert NA percentages to a data frame
na_df <- data.frame(
  Column = names(na_percentage),
  NA_Percentage = na_percentage
)

# Create bar plot
ggplot(na_df, aes(x = Column, y = NA_Percentage)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    title = "Percentage of NA Values per Column",
    x = "Columns",
    y = "Percentage (%)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1) # Rotate x-axis labels
  )

# Print the number of rows in the full dataset
print(paste("Number of rows in full dataset =", nrow(data)))

# Print the number of rows after NA removal
print(paste("Number of rows after NA removal on entire dataset =", nrow(na.omit(data))))
```

# PCA
### Choose the Variables to be Used in the PCA
This saves the variables to be used in the PCA as an object. The variables used are easily altered by commenting them in or out. Only numerical variables are considered as candidates as PCA can only use numerical variables. Where two variables describe the same trait, one of them was removed. This was done in order to avoid bias in the PCA by over emphasising particular traits.

"Clutch.Frequency" describes whether a species is single or multiple brooded (i.e lays a clutch once or several times during the breeding season) and "Clutches.Per.Year" is the number of clutches laid per year. These two variables describe the same trait (clutches per year/breeding season) in different ways. Clutch frequency describes it as between 0.5 (which we have guessed means not certain to produce a clutch every breeding season), 1 (one clutch per season) or 2 (more than 1 clutch per breeding season). In contrast clutches per year simply states the number of clutches produced each year. As this is the more high definition variable it was decided to keep it in the analysis instead of clutch frequency.

"F.SVL.at.maturity..mm" describes the same trait as "Mean.F.SVL.adults..mm.". A correlation of these two variables revealed a 97% correlation. Therefore, "F.SVL.at.maturity..mm." was randomly chosen to be removed.

Sample sizes and standard deviations were removed from the analysis as they were not thought to be relevant. 
```{r}
variables_to_use_PCA <- c(
  ### Location
  #"Longitude",
  #"Latitude",
  
  ### Means and Numerical
  "Average.Female.adult.weight..g.",
  "Mean.F.SVL.adults..mm.",
  #"F.SVL.at.maturity..mm.",
  "Offspring.SVL..mm.",
  "Mean.Clutch.Size",
  "Clutches.per.year",
  #"Clutch.Frequency",
  "RCM"
  
  ### Standard Deviations and Sample Sizes
  #"SD.Female.adult.weight..g.",
  #"SD.F.SVL.adults..mm.",
  #"Sample.Size.Female.adult.weight",
  #"Sample.size.Mean.F.SVL.adults",
)
```

### Investigation of the Similar Variables
```{r}
# Are clutch frequency and clutches per year the same?  
cor(data$Clutch.Frequency, data$Clutches.per.year, use = "complete.obs") # Correlation of the two variables
unique(data$Clutch.Frequency) # Unique values of clutch frequency
unique(data$Clutches.per.year) # Unique values of clutches per year

plot(data$Clutch.Frequency~data$Clutches.per.year)

# Are "F.SVL.at.maturity..mm." and "Mean.F.SVL.adults..mm." the same?
cor(data$F.SVL.at.maturity..mm., data$Mean.F.SVL.adults..mm., use="complete.obs")
plot(data$F.SVL.at.maturity..mm.~data$Mean.F.SVL.adults..mm.)
```

### Create New Dataset retaining only the PCA variables
Here a new subset of the data is created for the PCA. The PCA data was cleaned of all rows containing NA's and standardised such that each variable has mean 0 and variance 1 (which allows for comparison of variables on different scales in the PCA). A second data set was created containing the rows retained after NA omition of the PCA data, but containing all the original variables. This is later used to color the PCA visualisations with categorical variables.
```{r}
# Create the new PCA dataset
data_PCA <- data %>%
  select(any_of(variables_to_use_PCA)) %>% # Filter for the variables to be used in the PCA
  na.omit() %>% # Omit NA's
  mutate_all(.funs = scale) # Scale the data for the PCA

# Create ancillary dataset with all original variables but filtered to keep only rows present in the PCA dataset
# This will be used for coloration of the PCA graphs with categorical variables
data_PCA_all_vars <- data %>%
  filter(row_number() %in% rownames(data_PCA))
```

### Visualise with a Correlation
This shows the correlation of each variable pairs in PCA data.
```{r}
### CorrPlot
data_PCA %>% 
  cor(use="pairwise.complete.obs") %>% # Calculate the empirical correlation matrix
  corrplot() # Then graph this matrix
```

### Run the PCA
```{r}
# Run the PCA
result_pca <- PCA(data_PCA, 
               scale.unit = TRUE, # Option to center and scale data (useless here)
               ncp = 18, # Number of components to keep (here, all)
               graph = FALSE)

# Results of the PCA
result_pca$eig
result_pca$var

# Plot the Scree Plot to Show Variance Explained by Each Dimension
fviz_eig(result_pca, choice = "variance")

# Plot the Correlation of Each Variable and the Dimension
corrplot(result_pca$var$cor)
```

### Plot the Individuals with the Variable Arrows and the Categorical Coloring
By altering the object Color_label we were able to check the effect of coloring with each of the four main categorical variables (Distribution, Habitat type, Foraging mode and Reproductive type). It was found that distribution produced the clearest clustering with the least noise. Therefore distribution was taken forward as a potential categorical variable which can account for much of the variation.
```{r}
Color_label <- "Distribution"

### Plot all four on one plot
# Generate the plots
p1 <- fviz_pca_biplot(result_pca, axes = c(1, 2), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black")
p2 <- fviz_pca_biplot(result_pca, axes = c(1, 3), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black")
p3 <- fviz_pca_biplot(result_pca, axes = c(2, 3), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black")
p4 <- fviz_pca_biplot(result_pca, axes = c(3, 4), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black")

# Arrange the plots in a grid
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

# K-Means Clustering
### Create Clusters for 2-5 Groups Out of Curiosity
```{r}
# Create Clusters For Four Groups
first_kmeans <- data_PCA %>% 
  kmeans(centers = 4)

# Within Group Inertia
first_kmeans$tot.withinss

kmeans_5groups<- data_PCA %>%
  kmeans(centers = 5)

kmeans_4groups<- data_PCA %>%
  kmeans(centers = 4)

kmeans_3groups<- data_PCA %>%
  kmeans(centers=3)

kmeans_2groups <- data_PCA %>%
  kmeans(centers=2)

# Extract the Clustering Groups
clusters_5groups <- kmeans_5groups$cluster
clusters_4groups <- kmeans_4groups$cluster
clusters_3groups <- kmeans_3groups$cluster
clusters_2groups <- kmeans_2groups$cluster
```

### Plot a Comparison of the 2 Group K-Means Clustering vs The Coloration Applied by the "Distribution" Variable
The k-means method found clusters which visually seem very similar to those applied by the Distribution category.
```{r}
# Define a color palette for consistency
custom_palette <- c("red", "blue", "green", "orange")

# Save each of the plots 
p1A <- fviz_pca_ind(result_pca, axes = c(1, 2), col.ind = as.factor(clusters_2groups), col.var = "black", title = "Coloration from K-Means Clustering")
p1B <- fviz_pca_ind(result_pca, axes = c(1, 2), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black", title = "Coloration from Distribution")
p2A <- fviz_pca_ind(result_pca, axes = c(1, 3), col.ind = as.factor(clusters_2groups), col.var = "black", title = "Coloration from K-Means Clustering")
p2B <- fviz_pca_ind(result_pca, axes = c(1, 3), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black", title = "Coloration from Distribution")
p3A <- fviz_pca_ind(result_pca, axes = c(2, 3), col.ind = as.factor(clusters_2groups), col.var = "black", title = "Coloration from K-Means Clustering")
p3B <- fviz_pca_ind(result_pca, axes = c(2, 3), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black", title = "Coloration from Distribution")
p4A <- fviz_pca_ind(result_pca, axes = c(3, 4), col.ind = as.factor(clusters_2groups), col.var = "black", title = "Coloration from K-Means Clustering")
p4B <- fviz_pca_ind(result_pca, axes = c(3, 4), col.ind = data_PCA_all_vars[[Color_label]], col.var = "black", title = "Coloration from Distribution")

# Plot each axis combination
all_4_plots<-grid.arrange(p1A, p1B, p2A, p2B, p3A, p3B, p4A, p4B, nrow = 4, ncol=2)

# Save a ggplot object
ggsave("all4dimensionsclustering.png", plot = all_4_plots, width = 11, height = 15.18, units = "in", dpi = 500)
```
# Unsupervised Learning
##Split Data
### Specify Variables to Use
```{r}
# Specify the Variables to Use in the Random Forests Analysis
variables_to_use_RF <- c(
  ### Categorical Variables
  #"Species",
  #"Genus",
  #"Family",
  #"Population",
  "Mode.of.reproduction",
  #"Source",
  "Distribution",
  "Prefered.Habitat.Type",
  "Foraging.Mode",
  "RCM",
  
  ### Location
  #"Longitude",
  #"Latitude",
  
  ### Numerical Variables
  "Average.Female.adult.weight..g.",
  "Mean.F.SVL.adults..mm.",
  "F.SVL.at.maturity..mm.",
  "Offspring.SVL..mm.",
  "Mean.Clutch.Size",
  "Clutches.per.year",
  "Clutch.Frequency",
  "RCM"
  
  ### Standard Deviations and Sample Sizes
  #"SD.F.SVL.adults..mm.",
  #"Sample.size.Mean.F.SVL.adults",
  #"SD.Female.adult.weight..g.",
  #"Sample.Size.Female.adult.weight"
)
```


###Filter Data Variables
Filter out the data to only keep variables to be used in the random forests and KNN analyses.
Remove Nas.
```{r}
data_RF <- data %>%
  select(any_of(variables_to_use_RF)) %>% # Select variables
  na.omit() # Remove NAs
```

### Initial Split
Split the data such that 80% is used for the training and 20% is unseen.
```{r}
split_data <- data_RF %>%
  initial_split(prop = 0.8, strata = "Distribution")

# Train and Test Data
train_data <- analysis(split_data)
test_data <- assessment(split_data)
```

## Random Forests
### Choose the Optimum Hyperparameter 'mtry'.
This chunk enables us to determine the optimum value of the hyperparameter mtry. Here it is found to be at the highest for the mtry values 4, 5 and 6.
```{r}
# Initialisation
params_CV <- trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 5)

predictor_forest <- caret::train(Distribution ~ ., 
                                 data = data_RF, 
                                 method = "rf", 
                                 metric = "Accuracy", 
                                 trControl = params_CV, 
                                 tuneGrid = expand.grid(mtry = 2:12))

as.data.frame(predictor_forest$results) %>% 
  ggplot(aes(x = mtry, y = Accuracy)) +
  geom_point()
```

### Create the RF
Use 5000 random trees and mtry value 4.
```{r}
forest <- randomForest(Distribution ~ ., # Formula for prediction
                      data = train_data, # Data for training
                      ntree = 5000, # Number of trees
                      maxnodes = 10, # Number of maximum leaves for each tree
                      mtry = 4, # Number of variables for each tree
                      importance = TRUE) # Computation of importance
```

### Test the Accuracy
Accuracy found to be 
```{r}
# Fast way of computing accuracy with the pipe operator %>% 
predict(forest, newdata = select(test_data, - Distribution)) %>% 
  table(prediction = ., truth = test_data$Distribution) %>% 
  {sum(diag(.)) / sum(.)}
```

### Find the Variable Importance
Found the Mean Clutch Size caused the highest mean decrease in accuracy and mean decrease in the Gini coefficient of impurity. Therefore it is the variable contributing the most to the classification. 

```{r}
varImpPlot(forest)
```

## KNN 
### 





