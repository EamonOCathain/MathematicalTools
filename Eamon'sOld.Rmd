---
title: "R Notebook"
output: html_notebook
---
# Classification Trees
### Maximal Tree
```{r}
maximal_tree <- rpart(Prefered.Habitat.Type ~ ., # Formula: we explain the variable sol with all the others
                   data = train_data,
                   method = "class", # Classification tree
                   control = rpart.control(minsplit = 2, # Minimal number of individuals in a node before cut
                                 cp = 0,  # Parameter of minimal complexity
                                 xval = 1)) # Number of cross-validation

rpart.plot(maximal_tree)
```

### Choose Complexity Parameter
```{r}
maximal_tree <- rpart(Prefered.Habitat.Type ~ ., 
                   data = train_data,
                   method = "class",  
                   control = rpart.control(minsplit = 2, 
                                           cp=0,
                                           xval = 100))

# Table of results
maximal_tree$cptable 
# xerror gives CV mean error, xstd gives its standard deviation

# We choose cp_optimal in the column "CP"
# The row index is the one for which the value in column "xerror" is minimal
cp_optimal <- maximal_tree$cptable[which.min(maximal_tree$cptable[, "xerror"]) , "CP"] # Column
cp_optimal
```
Optimal CP = 0.01639344

### Prune Maximal Tree for Optimal
```{r}
optimal_tree <- prune(maximal_tree, cp = cp_optimal)
rpart.plot(optimal_tree)
```
### Test the Optimal Tree
```{r}
# Predictions on test set
# (not used for training)
predictions = predict(optimal_tree, newdata = select(test_data, - Prefered.Habitat.Type)) 
predicted = apply(predictions,1,which.max)
predicted_factor = factor(x = predicted, levels = c(1,2,3), labels = c("Alluvial","Dunaire","GrÃ¨s"))

# Confusion matrix
confusion_matrix = table(prediction = predicted_factor, truth = test_data$Prefered.Habitat.Type) 
confusion_matrix 
```


```{r}
# Make a list of all the clusterings
list_clusters <- list(clusters_2groups, clusters_3groups, clusters_4groups, clusters_5groups)

# Loop through each group and plot the four main axis combinations
for (number_groups in list_clusters) {
  # Generate the Plots
  p1 <- fviz_pca_biplot(result_pca, axes = c(1, 2), col.ind = number_groups)
  p2 <- fviz_pca_biplot(result_pca, axes = c(1, 3), col.ind = number_groups)
  p3 <- fviz_pca_biplot(result_pca, axes = c(2, 3), col.ind = number_groups)
  p4 <- fviz_pca_biplot(result_pca, axes = c(3, 4), col.ind = number_groups)
  
  # Arrange the plots in a grid
  grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
}
```

### Plot the Tricor
```{r}
table_reduced_CCA <- result_CCA$CCA$u
## Correlations
corr_pc_chemi = cor(table_reduced_CCA, mean_latitude_df[, c("mean_latitude")])
corrplot(corr_pc_chemi)
```

```{r}
# Create a list of all possible values to test
list_variables_RF_numeric <- list(
  "Mean.F.SVL.adults..mm.",
  "Offspring.SVL..mm.",
  "Mean.Clutch.Size",
  "Clutches.per.year",
  "RCM") 

RF_iterations_df <- data.frame(
  variable = character(),
  mtry = numeric(),
  accuracy = numeric(),
  stringsAsFactors = FALSE
)

counter <- 0

for (var in list_variables_RF){
  
counter <- counter + 1   
  
split_data <- data_RF %>%
  initial_split(prop = 0.8, strata = var)

# Train and Test Data
train_data <- analysis(split_data)
test_data <- assessment(split_data)

# Initialisation
params_CV <- trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 5)

predictor_forest <- caret::train(Distribution ~ ., 
                                 data = data_RF, 
                                 method = "rf", 
                                 metric = "RMSE", 
                                 trControl = params_CV, 
                                 tuneGrid = expand.grid(mtry = 2:12))

best_mtry <- predictor_forest$results$mtry[which.max(predictor_forest$results$RMSE)]
highest_RMSE <- max(predictor_forest$results$RMSE)

forest <- randomForest(all_of(var) ~ ., # Formula for prediction
                      data = train_data, # Data for training
                      ntree = 5000, # Number of trees
                      maxnodes = 10, # Number of maximum leaves for each tree
                      mtry = best_mtry, # Number of variables for each tree
                      importance = TRUE) # Computation of importance

# Fast way of computing accuracy with the pipe operator %>% 
Accuracy <- predict(forest, newdata = select(test_data, - Distribution)) %>% 
  table(prediction = ., truth = test_data$Distribution) %>% 
  {sum(diag(.)) / sum(.)}

RF_iterations_df[counter, "variable"] <- var
RF_iterations_df[counter, "mtry"] <- best_mtry
RF_iterations_df[counter, "accuracy"] <- Accuracy
}
```