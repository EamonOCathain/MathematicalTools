---
title: "BeginAnalysis"
author: "Eamon"
date: "2024-12-18"
output: html_document
---
# Initialisation
## Set Up
### Load Libraries
```{r}
library(tidyverse)
library(corrplot) # For correlation plots
library(FactoMineR) # For PCA
library(factoextra) # For PCA plots
library(vegan) # For CCA
library(ggplot2)
library(rsample) # For sampling of data (split, analysis,..)
library(rpart)
library(rpart.plot)
library(randomForest)
```

### 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Clear environment
```{r}
rm(list = ls())
```


### Specify the FilePath and Read CSV
```{r}
path <- "/Users/eamon/Desktop/MathToolsProject/R_Project/MathematicalTools/lizard.csv"

# Read a CSV file
data <- read.csv(path)

# View the first few rows of the data
data
```

## Cleaning
### Change Variables Types
```{r}
# Check the type of each column
sapply(data, class)

# Convert SD.Female.adult.weight..g. to numeric
data$SD.Female.adult.weight..g. <- as.numeric(data$SD.Female.adult.weight..g.)

# Convert categorical columns to factors
data$Species <- as.factor(data$Species)
data$Genus <- as.factor(data$Genus)
data$Family<- as.factor(data$Family)
data$Mode.of.reproduction <- as.factor(data$Mode.of.reproduction)
data$Foraging.Mode <- as.factor(data$Foraging.Mode)
data$Distribution <- as.factor(data$Distribution)
data$Prefered.Habitat.Type <- as.factor(data$Prefered.Habitat.Type)
```

### Check for and Remove Duplicates
```{r}
# Check for Duplicates
duplicates <- data[duplicated(data), ]
print("Duplicate Rows:")
print(duplicates)

# Remove duplicates
data <- data[!duplicated(data), ]
```

### Calculate Percentage of Each Variable Which Are NA
```{r}
# Calculate percentage of NA values for each column
na_percentage <- colSums(is.na(data)) / nrow(data) * 100

# Convert NA percentages to a data frame
na_df <- data.frame(
  Column = names(na_percentage),
  NA_Percentage = na_percentage
)

# Create bar plot
ggplot(na_df, aes(x = Column, y = NA_Percentage)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    title = "Percentage of NA Values per Column",
    x = "Columns",
    y = "Percentage (%)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1) # Rotate x-axis labels
  )
```

### Remove Unneeded Variables
Here we removed the variables that we didn't plan to use in the PCA. 
This was done before filtering out NAs to avoid unneccessary data loss.
All rows which are missing RCM (NA) are also missing mean female adult weight (which makes sense as they're related).
```{r}
data_filtered <- data %>%
  select(-Sample.Size.Female.adult.weight, -Sample.size.Clutch.Size., -Sample.size.Mean.F.SVL.adults, SD.Female.adult.weight..g., -SD.F.SVL.adults..mm., -RCM)
```

## NAs
### Check Amount of Data Loss If All NAs Removed
```{r}
data_clean <- na.omit(data_filtered)
nrow(data)
# Too many rows lost if all NAs removed
nrow(data_clean)
```

# PCA
### Select Only Variables of Interest
```{r}
data_selected <- data_clean %>%
  select_if(is.numeric) %>%
  select(-Latitude, -Longitude)
```

### Scale the Data
```{r}
data_scaled <- data_selected %>% 
  #Only numeric columns are selected
  mutate_all(.funs = scale)
head(data_scaled)
```

### Visualise with a Correlation
```{r}
### CorrPlot
data_scaled %>% 
  cor(use="pairwise.complete.obs") %>% # Calculate the empirical correlation matrix
  corrplot() # Then graph this matrix
```

### Run the PCA
```{r}
result_pca <- PCA(data_scaled, 
               scale.unit = TRUE, # Option to center and scale data (useless here)
               ncp = 18, # Number of components to keep (here, all)
               graph = FALSE)
```

### Result of PCA
```{r}
names(result_pca) # It's a list
result_pca
```

### Percentage of Information Retained
```{r}
fviz_eig(result_pca, choice = "variance")
```

### Visualise the PCA
```{r}
# Representation in the first principal plane
fviz_pca_var(result_pca,
             axes = c(1, 2)) # Number of axes to represent 
fviz_pca_var(result_pca,
             axes = c(1, 3)) # Number of axes to represent 
fviz_pca_var(result_pca,
             axes = c(2, 3)) # Number of axes to represent 
fviz_pca_var(result_pca,
             axes = c(1, 4))
```

### Extract Categorical Variables
```{r}
distribution_series <- data_clean$Distribution
foraging_mode <- data_clean$Foraging.Mode
habitat_series <- data_clean$Prefered.Habitat.Type
reproduction_mode_series <- data_clean$Mode.of.reproduction
family <- data_clean$Family
clutch_freq_series<-data_clean$Clutch.Frequency
```

### Represent Individuals
```{r}
# Representation in the first principal plane
fviz_pca_ind(result_pca,
             axes = c(1, 2),
             habillage = family) # Number of axes to represent
```

```{r}
# Representation in the first principal plane
fviz_pca_biplot(result_pca,
             axes = c(1, 2),
             col.ind = clutch_freq_series) # Number of axes to represent
```

```{r}
head(data_clean)
```


# Unsupervised Learning
##Split Data
###Filter Data Variables
```{r}
data_filtered <- data_clean %>%
  select(-)
  select(is.numeric, Clutch.Frequency, -SD.Female.adult.weight..g., -Latitude, -Longitude)
head(data_filtered)
```



### Initial Split 
```{r}
split_data <- data_filtered %>%
  initial_split(prop = 0.8, strata = "Prefered.Habitat.Type")

# Train and Test Data
train_data <- analysis(split_data)
test_data <- assessment(split_data)
```

### Maximal Tree
```{r}
maximal_tree <- rpart(Prefered.Habitat.Type ~ ., # Formula: we explain the variable sol with all the others
                   data = train_data,
                   method = "class", # Classification tree
                   control = rpart.control(minsplit = 2, # Minimal number of individuals in a node before cut
                                 cp = 0,  # Parameter of minimal complexity
                                 xval = 1)) # Number of cross-validation

rpart.plot(maximal_tree)
```

### Choose Complexity Parameter
```{r}
maximal_tree <- rpart(Prefered.Habitat.Type ~ ., 
                   data = train_data,
                   method = "class",  
                   control = rpart.control(minsplit = 2, 
                                           cp=0,
                                           xval = 100))

# Table of results
maximal_tree$cptable 
# xerror gives CV mean error, xstd gives its standard deviation

# We choose cp_optimal in the column "CP"
# The row index is the one for which the value in column "xerror" is minimal
cp_optimal <- maximal_tree$cptable[which.min(maximal_tree$cptable[, "xerror"]) , "CP"] # Column
cp_optimal
```
Optimal CP = 0.01639344

### Prune Maximal Tree for Optimal
```{r}
optimal_tree <- prune(maximal_tree, cp = cp_optimal)
rpart.plot(optimal_tree)
```
### Test the Optimal Tree
```{r}
# Predictions on test set
# (not used for training)
predictions = predict(optimal_tree, newdata = select(test_data, - Prefered.Habitat.Type)) 
predicted = apply(predictions,1,which.max)
predicted_factor = factor(x = predicted, levels = c(1,2,3), labels = c("Alluvial","Dunaire","GrÃ¨s"))

# Confusion matrix
confusion_matrix = table(prediction = predicted_factor, truth = test_data$Prefered.Habitat.Type) 
confusion_matrix 
```

# Random Forests
### Create the RF
```{r}
forest <- randomForest(Clutch.Frequency ~ ., # Formula for prediction
                      data = train_data, # Data for training
                      ntree = 5000, # Number of trees
                      maxnodes = 10, # Number of maximum leaves for each tree
                      mtry = 4, # Number of variables for each tree
                      importance = TRUE) # Computation of importance
```

### Test the Accuracy
```{r}
# Fast way of computing accuracy with the pipe operator %>% 
predict(forest, newdata = select(test_data, - Clutch.Frequency)) %>% 
  table(prediction = ., truth = test_data$Clutch.Frequency) %>% 
  {sum(diag(.)) / sum(.)}
```


